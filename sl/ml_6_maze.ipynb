{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d722dc9f-6b5d-41ef-a0d4-ef4a0551a6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q-Table:\n",
      "[[[-0.67108401 -0.0434062  -0.6596749  -0.56884186]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.25556198  0.06024417 -0.21397658  0.8       ]\n",
      "  [-0.1        -0.10965367  0.05219     1.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.32962058  0.062882   -0.56763977 -0.70338836]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.62       -0.02746457 -0.37665468 -0.37357948]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.18731646 -0.57341646 -0.44370292  0.18098   ]\n",
      "  [-0.46888239 -0.41653968 -0.09074595  0.3122    ]\n",
      "  [ 0.458      -0.16064348 -0.10655847 -0.13384147]\n",
      "  [-0.19709412 -0.192439   -0.04124843 -0.12921187]\n",
      "  [-0.19619388 -0.13029543 -0.13229608 -0.199     ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.10023423 -0.14700515 -0.199      -0.199     ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.132102   -0.13110202 -0.19436944 -0.28599404]]\n",
      "\n",
      " [[-0.199      -0.199      -0.199      -0.14959518]\n",
      "  [-0.199      -0.19       -0.14811722 -0.14914585]\n",
      "  [-0.1493148  -0.29084011 -0.14777295 -0.199     ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.13200832 -0.19618389 -0.199      -0.199     ]]]\n",
      "Path taken by the agent: [(0, 0), (1, 0), (2, 0), (2, 1), (2, 2), (1, 2), (0, 2), (0, 3), (0, 4)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the maze\n",
    "maze = [\n",
    "    [0, -1, 0, 0, 1],\n",
    "    [0, -1, 0, -1, -1],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [-1, -1, 0, -1, 0],\n",
    "    [0, 0, 0, -1, 0]\n",
    "]\n",
    "\n",
    "start = (0, 0)  # Starting point\n",
    "goal = (0, 4)   # Goal point\n",
    "\n",
    "# Map actions to numbers (0, 1, 2, 3)\n",
    "# 0 = up, 1 = down, 2 = left, 3 = right\n",
    "action_dict = {\n",
    "    0: (-1, 0),  # up\n",
    "    1: (1, 0),   # down\n",
    "    2: (0, -1),  # left\n",
    "    3: (0, 1)    # right\n",
    "}\n",
    "\n",
    "# Initialize Q-table with zeros (maze dimensions x number of actions)\n",
    "q_table = np.zeros((len(maze), len(maze[0]), 4))\n",
    "\n",
    "alpha = 0.1     # Learning rate\n",
    "gamma = 0.9     # Discount factor\n",
    "epsilon = 0.1   # Exploration rate\n",
    "episodes = 1000 # Number of episodes\n",
    "\n",
    "def is_valid_position(position):\n",
    "    row, col = position\n",
    "    return 0 <= row < len(maze) and 0 <= col < len(maze[0]) and maze[row][col] != -1\n",
    "\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        # Random action (0, 1, 2, or 3)\n",
    "        return random.randint(0, 3)\n",
    "    else:\n",
    "        row, col = state\n",
    "        # Exploit the best action (max Q-value)\n",
    "        return np.argmax(q_table[row, col])  # Exploit the best action based on Q-table\n",
    "\n",
    "# Q-learning\n",
    "for episode in range(episodes):\n",
    "    state = start\n",
    "    while state != goal:\n",
    "        row, col = state\n",
    "        action = choose_action(state)\n",
    "\n",
    "        # Ensure valid action is chosen (safety check)\n",
    "        if action not in action_dict:\n",
    "            print(f\"Invalid action: {action}, using 0 (up) instead.\")\n",
    "            action = 0  # Default to 0 (up) in case of invalid action.\n",
    "\n",
    "        move = action_dict[action]\n",
    "        next_state = (row + move[0], col + move[1])\n",
    "\n",
    "        if not is_valid_position(next_state):\n",
    "            reward = -1  # Penalty for hitting a wall\n",
    "            next_state = state  # Stay in the same position\n",
    "        elif next_state == goal:\n",
    "            reward = 1  # Reward for reaching the goal\n",
    "        else:\n",
    "            reward = -0.1  # Small penalty for each move\n",
    "\n",
    "        # Update Q-value\n",
    "        next_row, next_col = next_state\n",
    "        best_next_action = np.max(q_table[next_row, next_col])\n",
    "        q_table[row, col, action] += alpha * (reward + gamma * best_next_action - q_table[row, col, action])\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "    # Decrease exploration rate over time\n",
    "    epsilon = max(0.01, epsilon * 0.99)\n",
    "\n",
    "# Print the trained Q-table\n",
    "print(\"Trained Q-Table:\")\n",
    "print(q_table)\n",
    "\n",
    "# Find the path using the trained Q-table\n",
    "state = start\n",
    "path = [state]\n",
    "while state != goal:\n",
    "    row, col = state\n",
    "    action = np.argmax(q_table[row, col])  # Choose the best action based on Q-values\n",
    "    move = action_dict[action]\n",
    "    next_state = (row + move[0], col + move[1])\n",
    "    if not is_valid_position(next_state):\n",
    "        break\n",
    "    state = next_state\n",
    "    path.append(state)\n",
    "\n",
    "# Print the path taken by the agent\n",
    "print(\"Path taken by the agent:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ba0bd9-f507-41d9-8b59-75283facf04f",
   "metadata": {},
   "source": [
    "A **maze** is a grid-based environment that contains paths, walls, and sometimes a goal. It’s a classic setting for teaching and testing **Reinforcement Learning (RL)** concepts, as it provides a controlled space with clear objectives and obstacles. In the maze environment, the **agent** (like a robot or virtual entity) must navigate from a **starting position** to a **goal position** while avoiding walls or obstacles. \n",
    "\n",
    "### Purpose of the Maze in Reinforcement Learning\n",
    "\n",
    "In Reinforcement Learning, the **purpose of the maze environment** is to allow an agent to learn **how to make decisions** to reach a target. Since each cell in the maze represents a state, the agent must decide which direction to move next. By **exploring** different paths and learning from the **feedback** (rewards or penalties), the agent can identify the **optimal path** to reach the goal.\n",
    "\n",
    "### How Reinforcement Learning Works in a Maze\n",
    "\n",
    "The maze environment offers a straightforward scenario for RL, where:\n",
    "- **The agent** interacts with the maze, making choices at each step.\n",
    "- **The goal** is to maximize rewards (like reaching the endpoint with the fewest steps or avoiding obstacles).\n",
    "- The agent learns a **policy** (strategy) to navigate the maze efficiently.\n",
    "\n",
    "In the RL approach, the agent:\n",
    "1. **Explores** different paths and experiences various outcomes.\n",
    "2. **Learns** from rewards (positive when moving closer to the goal, negative when hitting a wall or backtracking).\n",
    "3. **Adapts** its decisions over time to find the best route, effectively solving the maze with an optimal policy.\n",
    "\n",
    "Using this, let’s walk through the implementation steps:\n",
    "\n",
    "1. **Define the Maze Environment**: Represent the maze as a grid where each cell can be an open path, an obstacle, or the goal.\n",
    "2. **Implement Q-Learning**: Use a popular RL technique, Q-Learning, where the agent learns the best actions by updating Q-values (estimations of the quality of actions).\n",
    "3. **Train the Agent**: Run simulations for the agent to explore and find optimal paths.\n",
    "4. **Test the Agent’s Policy**: Once trained, the agent should be able to navigate to the goal efficiently. \n",
    "\n",
    "Would you like to proceed with setting up the code for implementing this in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d617f-718e-4053-8bf7-322880fb61a1",
   "metadata": {},
   "source": [
    "This code demonstrates **Q-Learning** in a simple maze environment, where the agent learns to navigate from a **starting point** to a **goal** while avoiding obstacles. Here’s an overview of how the code is structured and how it enables the agent to learn and find an optimal path:\n",
    "\n",
    "### Code Breakdown\n",
    "\n",
    "1. **Maze Setup**:\n",
    "   - The maze is represented as a 2D list where:\n",
    "     - `0` indicates an open path.\n",
    "     - `-1` indicates a wall or obstacle that the agent cannot pass.\n",
    "     - `1` at `(0, 4)` represents the goal.\n",
    "   - **Starting point** is `(0, 0)`, and the **goal** is `(0, 4)`.\n",
    "\n",
    "2. **Action Space**:\n",
    "   - The agent can move in four directions:\n",
    "     - `0` for up, `1` for down, `2` for left, and `3` for right.\n",
    "   - Each action has a corresponding movement vector in `action_dict`.\n",
    "\n",
    "3. **Q-Table Initialization**:\n",
    "   - The Q-table is initialized to store **Q-values** for each state-action pair. It has dimensions matching the maze size (`[rows][columns][4 actions]`).\n",
    "\n",
    "4. **Learning Parameters**:\n",
    "   - `alpha` (learning rate): Determines how much new information overrides the old.\n",
    "   - `gamma` (discount factor): Controls the value of future rewards.\n",
    "   - `epsilon` (exploration rate): Probability of choosing a random action to encourage exploration.\n",
    "\n",
    "5. **Q-Learning Algorithm**:\n",
    "   - For each episode, the agent starts from the beginning.\n",
    "   - **Choose an action**: Based on `epsilon`, the agent either explores randomly or exploits the best action (highest Q-value) for the current state.\n",
    "   - **Update Q-values**: The Q-value for each state-action pair is updated based on the received reward and the estimated best future reward.\n",
    "   - **Decrease `epsilon`** gradually to reduce exploration over time, allowing the agent to rely more on learned behavior.\n",
    "\n",
    "6. **Training Process**:\n",
    "   - The agent navigates the maze for `episodes` number of times, gradually improving its path by adjusting Q-values based on rewards.\n",
    "   - Rewards include:\n",
    "     - `-1` for hitting a wall.\n",
    "     - `1` for reaching the goal.\n",
    "     - `-0.1` for each move to encourage efficiency.\n",
    "\n",
    "7. **Path Extraction**:\n",
    "   - After training, the agent uses the learned Q-table to find the **optimal path** from the starting point to the goal.\n",
    "\n",
    "### Output\n",
    "- **Trained Q-Table**: Displays Q-values learned for each state-action combination.\n",
    "- **Path Taken by the Agent**: Shows the sequence of states the agent follows to reach the goal based on the Q-values learned during training.\n",
    "\n",
    "### Example Usage\n",
    "This implementation allows the agent to autonomously learn how to reach the goal efficiently, overcoming obstacles and learning from penalties. The path output helps visualize how well the agent learned from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3da7c1-2c9a-441b-aa31-36bcdfda2308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
