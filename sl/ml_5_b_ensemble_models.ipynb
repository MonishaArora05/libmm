{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f0e6dc-9323-4fc8-9d91-0cc667b93334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 1.00\n",
      "AdaBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "AdaBoost Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0 10]]\n",
      "Gradient Boosting Accuracy: 1.00\n",
      "Gradient Boosting Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Gradient Boosting Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0 10]]\n",
      "XGBoost Accuracy: 1.00\n",
      "XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "XGBoost Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0 10]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [13:06:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Full Code for Ensemble Learning (Iris Dataset)\n",
    "\n",
    "# Importing libraries for data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing libraries for model building and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Importing XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Importing Label Encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris_data = pd.read_csv(\"Iris.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "iris_data.head()\n",
    "\n",
    "# Check for missing values\n",
    "iris_data.isnull().sum()\n",
    "\n",
    "# Label encode the target variable (Species)\n",
    "le = LabelEncoder()\n",
    "iris_data['Species'] = le.fit_transform(iris_data['Species'])\n",
    "\n",
    "# Check the encoding\n",
    "iris_data['Species'].unique()\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = iris_data.drop('Species', axis=1)\n",
    "y = iris_data['Species']\n",
    "\n",
    "# Perform an 80/20 split for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize and train AdaBoost Classifier\n",
    "ada_model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_ada = ada_model.predict(X_test)\n",
    "\n",
    "# Initialize and train Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Initialize and train XGBoost Classifier\n",
    "xgb_model = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate AdaBoost Classifier\n",
    "ada_accuracy = accuracy_score(y_test, y_pred_ada)\n",
    "ada_report = classification_report(y_test, y_pred_ada)\n",
    "ada_cm = confusion_matrix(y_test, y_pred_ada)\n",
    "\n",
    "print(f\"AdaBoost Accuracy: {ada_accuracy:.2f}\")\n",
    "print(\"AdaBoost Classification Report:\\n\", ada_report)\n",
    "print(\"AdaBoost Confusion Matrix:\\n\", ada_cm)\n",
    "\n",
    "# Evaluate Gradient Boosting Classifier\n",
    "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "gb_report = classification_report(y_test, y_pred_gb)\n",
    "gb_cm = confusion_matrix(y_test, y_pred_gb)\n",
    "\n",
    "print(f\"Gradient Boosting Accuracy: {gb_accuracy:.2f}\")\n",
    "print(\"Gradient Boosting Classification Report:\\n\", gb_report)\n",
    "print(\"Gradient Boosting Confusion Matrix:\\n\", gb_cm)\n",
    "\n",
    "# Evaluate XGBoost Classifier\n",
    "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "xgb_report = classification_report(y_test, y_pred_xgb)\n",
    "xgb_cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy:.2f}\")\n",
    "print(\"XGBoost Classification Report:\\n\", xgb_report)\n",
    "print(\"XGBoost Confusion Matrix:\\n\", xgb_cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af46d0b4-e2dc-4957-ba96-af6c76d38e4c",
   "metadata": {},
   "source": [
    "-\r\n",
    "\r\n",
    "### Observations:\r\n",
    "\r\n",
    "1. **AdaBoost (Adaptive Boosting)**:\r\n",
    "    - **What it is**: AdaBoost combines many weak models (simple decision trees) to make one strong model.\r\n",
    "    - **How it works**: It trains each new model to fix the mistakes made by the previous ones. It focuses more on the data points that were misclassified.\r\n",
    "    - **Strengths**:\r\n",
    "        - Works well on simple problems.\r\n",
    "        - Can be a bit sensitive to noisy or incorrect data.\r\n",
    "    - **Weaknesses**:\r\n",
    "        - May not work well for complex problems.\r\n",
    "        - It can focus too much on mistakes, leading to overfitting.\r\n",
    "\r\n",
    "2. **Gradient Boosting**:\r\n",
    "    - **What it is**: Gradient Boosting builds models that focus on the errors of the previous ones. It improves on mistakes by learning from them.\r\n",
    "    - **How it works**: After each model is built, it corrects the errors from the last one, making the predictions better.\r\n",
    "    - **Strengths**:\r\n",
    "        - Works better on complex problems compared to AdaBoost.\r\n",
    "        - It can learn more complex patterns in the data.\r\n",
    "    - **Weaknesses**:\r\n",
    "        - It may overfit if not properly tuned (learning too much from the data and not generalizing well).\r\n",
    "        - Slower to run and more computationally expensive than AdaBoost.\r\n",
    "\r\n",
    "3. **XGBoost (Extreme Gradient Boosting)**:\r\n",
    "    - **What it is**: XGBoost is a faster and more efficient version of Gradient Boosting. It adds regularization to prevent overfitting and speeds up the learning process.\r\n",
    "    - **How it works**: It improves on Gradient Boosting by using optimizations and adding techniques like pruning trees and handling missing data.\r\n",
    "    - **Strengths**:\r\n",
    "        - Faster and more accurate than Gradient Boosting.\r\n",
    "        - Less likely to overfit because of regularization.\r\n",
    "    - **Weaknesses**:\r\n",
    "        - More complex and harder to tune.\r\n",
    "        - Requires more memory and computational power, especially for large datasets.\r\n",
    "\r\n",
    "### Evaluation Metrics:\r\n",
    "- **Accuracy**: Shows how many predictions were correct overall. XGBoost usually performs the best here.\r\n",
    "- **Classification Report**: Includes Precision, Recall, and F1-Score, which help in evaluating how well the model handles different types of errors.\r\n",
    "- **Confusion Matrix**: Shows where the model is making mistakes, like which classes it confuses the most.\r\n",
    "\r\n",
    "### Summary:\r\n",
    "- **AdaBoost**: Best for simple problems, but can struggle with noise in data.\r\n",
    "- **Gradient Boosting**: Works better on complex problems and improves over AdaBoost by focusing on fixing errors.\r\n",
    "- **XGBoost**: The best for speed and accuracy, especially on big datasets, but more complex to tune.\r\n",
    "\r\n",
    "### Additional Observations:\r\n",
    "- We expect **XGBoost** to have the highest accuracy due to its optimizations and regularization techniques, but **Gradient Boosting** and **AdaBoost** are also strong contenders.\r\n",
    "- Each model has its own strengths and trade-offs. AdaBoost is simpler and faster for smaller datasets, Gradient Boosting is more powerful for structured data, and XGBoost tends to be the most accurate for larger, more complex datasets.\r\n",
    "- **Based on the results**, we would choose the model that best fits the problem's needs (accuracy, computational effici\r\n",
    "--- \r\n",
    "\r\n",
    "You can now use this in your markdown file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f7218-ff6d-4895-89c7-2e37f9678a76",
   "metadata": {},
   "source": [
    "Here's an explanation of each step in the code:\n",
    "\n",
    "1. **Importing Libraries for Data Manipulation and Visualization**:\n",
    "    - `pandas` is imported to handle data in the form of DataFrames.\n",
    "    - `numpy` is imported for numerical computations.\n",
    "    - `matplotlib.pyplot` and `seaborn` are imported to create plots and visualize the data.\n",
    "\n",
    "2. **Importing Libraries for Model Building and Evaluation**:\n",
    "    - `train_test_split` from `sklearn.model_selection` is used to split the data into training and testing sets.\n",
    "    - `AdaBoostClassifier`, `GradientBoostingClassifier`, and `VotingClassifier` from `sklearn.ensemble` are ensemble models used for classification.\n",
    "    - `accuracy_score`, `classification_report`, and `confusion_matrix` from `sklearn.metrics` are used for evaluating the models' performance.\n",
    "\n",
    "3. **Importing XGBoost**:\n",
    "    - `XGBClassifier` from `xgboost` is imported to implement XGBoost, a highly efficient gradient boosting algorithm.\n",
    "\n",
    "4. **Importing Label Encoder**:\n",
    "    - `LabelEncoder` from `sklearn.preprocessing` is used to encode the target variable (Species) into numerical values since machine learning algorithms require numerical data.\n",
    "\n",
    "5. **Loading the Iris Dataset**:\n",
    "    - The Iris dataset is loaded using `pd.read_csv(\"Iris.csv\")`. This dataset contains information about flower species and their respective measurements.\n",
    "\n",
    "6. **Displaying the First Few Rows**:\n",
    "    - The `head()` function is used to display the first few rows of the dataset for inspection.\n",
    "\n",
    "7. **Checking for Missing Values**:\n",
    "    - The `isnull().sum()` function is used to check if there are any missing values in the dataset. This is crucial to ensure data quality.\n",
    "\n",
    "8. **Label Encoding the Target Variable (Species)**:\n",
    "    - The `LabelEncoder` is used to convert the categorical target variable (Species) into numeric labels (e.g., 0, 1, 2) so that they can be used in machine learning models.\n",
    "\n",
    "9. **Checking the Encoding**:\n",
    "    - The `unique()` function is used to check the unique values in the `Species` column after encoding, ensuring that the label encoding worked correctly.\n",
    "\n",
    "10. **Splitting the Data into Features (X) and Target (y)**:\n",
    "    - The dataset is divided into features (X) and the target variable (y). Features include the measurements, and the target variable is the encoded species.\n",
    "\n",
    "11. **Performing an 80/20 Split for Training and Testing**:\n",
    "    - The `train_test_split` function splits the data into training and testing sets. 80% of the data is used for training, and 20% is reserved for testing. The `stratify=y` argument ensures that the split maintains the class distribution in both sets.\n",
    "\n",
    "12. **Initializing and Training the AdaBoost Classifier**:\n",
    "    - The AdaBoost model is initialized with 100 estimators and trained using the training data (`X_train` and `y_train`). This model is an ensemble technique that combines weak classifiers to create a stronger classifier.\n",
    "\n",
    "13. **Predicting with AdaBoost**:\n",
    "    - The trained AdaBoost model is used to predict the target variable on the test data (`X_test`), and the predictions are stored in `y_pred_ada`.\n",
    "\n",
    "14. **Initializing and Training the Gradient Boosting Classifier**:\n",
    "    - Similar to AdaBoost, the Gradient Boosting model is initialized with 100 estimators and trained on the training data. This model also builds an ensemble of models, but it works by minimizing errors of previous models.\n",
    "\n",
    "15. **Predicting with Gradient Boosting**:\n",
    "    - The trained Gradient Boosting model is used to make predictions on the test data (`X_test`), and the results are stored in `y_pred_gb`.\n",
    "\n",
    "16. **Initializing and Training the XGBoost Classifier**:\n",
    "    - The XGBoost model is initialized with 100 estimators, and trained on the training data. XGBoost is an optimized version of Gradient Boosting with enhancements like regularization and faster computation.\n",
    "\n",
    "17. **Predicting with XGBoost**:\n",
    "    - The trained XGBoost model is used to make predictions on the test data, and the results are stored in `y_pred_xgb`.\n",
    "\n",
    "18. **Evaluating AdaBoost Classifier**:\n",
    "    - The performance of AdaBoost is evaluated using three metrics:\n",
    "        - **Accuracy**: The proportion of correct predictions.\n",
    "        - **Classification Report**: A detailed report including Precision, Recall, and F1-score.\n",
    "        - **Confusion Matrix**: A matrix showing the number of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "19. **Evaluating Gradient Boosting Classifier**:\n",
    "    - Similar to AdaBoost, the performance of the Gradient Boosting model is evaluated using accuracy, classification report, and confusion matrix.\n",
    "\n",
    "20. **Evaluating XGBoost Classifier**:\n",
    "    - Again, XGBoost’s performance is evaluated using accuracy, classification report, and confusion matrix.\n",
    "\n",
    "By following these steps, the code loads the Iris dataset, preprocesses the data, splits it into training and testing sets, applies three different ensemble learning models (AdaBoost, Gradient Boosting, and XGBoost), and evaluates their performance using standard classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daad3e16-7e82-4311-b166-5c6b8bce629d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
